import pytest

from deepeval.metrics import FaithfulnessMetric
from deepeval.test_case import LLMTestCase

from utils.LLMUtils import get_response_from_llm
from utils.TestDataUtils import load_data_sets


# Terminology
# - Faithfulness: The degree to which the answer provided by the LLM is faithful to the input question.
# - Threshold: A score above which the answer is considered faithful.
# - input: The question or prompt given to the LLM.
# - actual_output: The answer generated by the LLM.
# - retrieval_context: The context retrieved by the LLM to generate the answer.
@pytest.mark.parametrize("build_test_case",
                         load_data_sets("FaithfulnessDataFeed.json"),
                         indirect=True)
def test_faithfulness(build_test_case, model_name, expected_threshold):
    faithfulness_metric = FaithfulnessMetric(threshold=expected_threshold,
                                             model=model_name,
                                             include_reason=True)

    faithfulness_metric.measure(build_test_case)
    assert faithfulness_metric.score >= expected_threshold, (
        f"Score {faithfulness_metric.score:.2f} below threshold {expected_threshold}. Reason: {faithfulness_metric.reason}"
    )


@pytest.fixture
def build_test_case(request):
    test_data = request.param

    response_from_rag_llm = get_response_from_llm(test_data)
    # Extract the response and retrieved context
    retrieved_contexts = [doc["page_content"] for doc in response_from_rag_llm["retrieved_docs"]]

    test_case = LLMTestCase(
        input=test_data["input"],
        actual_output=response_from_rag_llm["answer"],
        retrieval_context=retrieved_contexts
    )

    return test_case
