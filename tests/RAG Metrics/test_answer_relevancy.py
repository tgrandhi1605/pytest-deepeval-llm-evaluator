import pytest

from deepeval import assert_test
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.test_case import LLMTestCase

from utils.LLMUtils import get_response_from_llm
from utils.TestDataUtils import load_data_sets

# Terminology
# - Answer Relevancy: The degree to which the answer provided by the LLM is relevant to the input question.
# - Threshold: A score above which the answer is considered relevant.
# - input: The question or prompt given to the LLM.
# - actual_output: The answer generated by the LLM.

@pytest.mark.parametrize("build_test_case",
                         load_data_sets("AnswerRelevancyDataFeed.json"),
                         indirect=True)
def test_answer_relevancy(build_test_case, expected_threshold, model_name):
    answer_relevancy_metric = AnswerRelevancyMetric(threshold=expected_threshold,
                                                    model=model_name,
                                                    include_reason=True)

    # You may need one of the below assertions. Choose that fits your needs

    # DeepEval in built assertion
    assert_test(build_test_case, [answer_relevancy_metric])

    # Generic assertion that pulls out details and prints the score and reason
    answer_relevancy_metric.measure(build_test_case)
    assert answer_relevancy_metric.score >= expected_threshold, (
        f"Score {answer_relevancy_metric.score:.2f} below threshold {expected_threshold}. Reason: {answer_relevancy_metric.reason}"
    )


@pytest.fixture
def build_test_case(request):
    test_data = request.param

    response_from_rag_llm = get_response_from_llm(test_data)

    test_case = LLMTestCase(
        input=test_data["input"],
        actual_output=response_from_rag_llm["answer"]
    )

    return test_case


