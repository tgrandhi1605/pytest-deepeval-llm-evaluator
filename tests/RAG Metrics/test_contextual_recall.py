import pytest

from deepeval.metrics import ContextualRecallMetric
from deepeval.test_case import LLMTestCase

from utils.LLMUtils import get_response_from_llm
from utils.TestDataUtils import load_data_sets


# Terminology
# - Contextual Recall: The degree to which the answer provided by the LLM is relevant to the input question.
# - Threshold: A score above which the answer is considered relevant.
# - input: The question or prompt given to the LLM.
# - expected_output: The expected answer from the test data.
# - actual_output: The answer generated by the LLM.
# - retrieval_context: The context retrieved by the LLM to generate the answer.
@pytest.mark.parametrize("build_test_case",
                         load_data_sets("ContextualRecallDataFeed.json"),
                         indirect=True)
def test_contextual_recall(build_test_case, model_name, expected_threshold):
    contextual_recall_metric = ContextualRecallMetric(threshold=expected_threshold,
                                                      model=model_name,
                                                      include_reason=True)
    contextual_recall_metric.measure(build_test_case)
    assert contextual_recall_metric.score >= expected_threshold, (
        f"Score {contextual_recall_metric.score:.2f} below threshold {expected_threshold}. Reason: {contextual_recall_metric.reason}"
    )


@pytest.fixture
def build_test_case(request):
    test_data = request.param

    response_from_rag_llm = get_response_from_llm(test_data)
    # Extract the response and retrieved context
    retrieved_contexts = [doc["page_content"] for doc in response_from_rag_llm["retrieved_docs"]]

    test_case = LLMTestCase(
        input=test_data["question"],
        expected_output=test_data["reference"],  # reference is the expected output from test_data
        actual_output=response_from_rag_llm["answer"],
        retrieval_context=retrieved_contexts
    )

    return test_case

