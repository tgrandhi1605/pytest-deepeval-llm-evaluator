import pytest
from deepeval.metrics import TextToImageMetric
from deepeval.test_case import MLLMTestCase, MLLMImage

from configs.conftest import expected_threshold
from utils.LLMUtils import get_response_from_llm
from utils.TestDataUtils import load_data_sets

# Terminology
# - TextToImage: The degree to which the image generated by the LLM is relevant to the input question.
# - Threshold: A score above which the image is considered relevant.
# - input: The question or prompt given to the LLM.
# - actual_output: The image generated by the LLM.
# - retrieval_context: The context retrieved by the LLM to generate the image.
@pytest.mark.parametrize("build_test_case",
                         load_data_sets("TextToImageDataFeed.json"),
                         indirect=True)
def test_text_to_image_metric(build_test_case,
                              model_name, expected_threshold):
    text_to_image_metric = TextToImageMetric(threshold=expected_threshold,
                                             model=model_name)

    text_to_image_metric.measure(build_test_case)
    assert text_to_image_metric.score >= expected_threshold, (
        f"Score {text_to_image_metric.score:.2f} below threshold {expected_threshold}. Reason: {text_to_image_metric.reason}"
    )


@pytest.fixture
def build_test_case(request):
    test_data = request.param

    response_from_rag_llm = get_response_from_llm(test_data)
    # Extract the response and retrieved context
    retrieved_contexts = [doc["page_content"] for doc in response_from_rag_llm["retrieved_docs"]]

    test_case = MLLMTestCase(
        input=test_data["question"],
        actual_output=[MLLMImage(url=test_data["reference"])],
        retrieval_context=retrieved_contexts
    )

    return test_case
